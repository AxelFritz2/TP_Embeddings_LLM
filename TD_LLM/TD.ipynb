{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ff10c4",
   "metadata": {},
   "source": [
    "# TP : LLM, Langchain et RAG\n",
    "## Installation des dépendances"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prérequis : \n",
    "- Mettre la clé d'API OpenAI en variable d'environnement\n",
    "- Installer les packages suivants : "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e053c2e3959006e"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\r\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\r\n",
      "Collecting openai\r\n",
      "  Downloading openai-1.109.0-py3-none-any.whl.metadata (29 kB)\r\n",
      "Collecting datasets\r\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m40.1/40.1 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: PyMuPDF in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (1.26.4)\r\n",
      "Collecting langchain_community\r\n",
      "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\r\n",
      "Collecting langchain_openai\r\n",
      "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\r\n",
      "Collecting langchain_text_splitters\r\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\r\n",
      "Collecting faiss-cpu\r\n",
      "  Downloading faiss_cpu-1.12.0-cp39-cp39-macosx_14_0_arm64.whl.metadata (5.1 kB)\r\n",
      "Collecting sentence-transformers\r\n",
      "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\r\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\r\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting langsmith>=0.1.17 (from langchain)\r\n",
      "  Downloading langsmith-0.4.30-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\r\n",
      "  Downloading pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m68.4/68.4 kB\u001B[0m \u001B[31m5.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting SQLAlchemy<3,>=1.4 (from langchain)\r\n",
      "  Downloading sqlalchemy-2.0.43-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.6 kB)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from langchain) (2.32.5)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from langchain) (6.0.2)\r\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\r\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from openai) (4.10.0)\r\n",
      "Collecting distro<2,>=1.7.0 (from openai)\r\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from openai) (0.28.1)\r\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\r\n",
      "  Downloading jiter-0.11.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: sniffio in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from openai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from openai) (4.15.0)\r\n",
      "Collecting filelock (from datasets)\r\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from datasets) (1.26.4)\r\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\r\n",
      "  Using cached pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\r\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: pandas in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from datasets) (2.3.2)\r\n",
      "Collecting xxhash (from datasets)\r\n",
      "  Using cached xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess<0.70.17 (from datasets)\r\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\r\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\r\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: packaging in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from datasets) (25.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from transformers) (2025.9.1)\r\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\r\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\r\n",
      "Collecting safetensors>=0.4.3 (from transformers)\r\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\r\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\r\n",
      "  Downloading aiohttp-3.12.15-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\r\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain_community)\r\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain_community)\r\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\r\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain_community)\r\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\r\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\r\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\r\n",
      "  Downloading tiktoken-0.11.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\r\n",
      "  Downloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from sentence-transformers) (1.6.1)\r\n",
      "Requirement already satisfied: scipy in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)\r\n",
      "Requirement already satisfied: Pillow in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from sentence-transformers) (11.3.0)\r\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Downloading frozenlist-1.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (18 kB)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Downloading multidict-6.6.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.3 kB)\r\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Downloading propcache-0.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\r\n",
      "  Downloading yarl-1.20.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (73 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.9/73.9 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\r\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\r\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\r\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: certifi in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\r\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.24.0->datasets)\r\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\r\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\r\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.1.17->langchain)\r\n",
      "  Downloading orjson-3.11.3-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m41.9/41.9 kB\u001B[0m \u001B[31m4.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\r\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\r\n",
      "  Downloading zstandard-0.25.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\r\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\r\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\r\n",
      "  Using cached pydantic_core-2.33.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.8 kB)\r\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\r\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain_community)\r\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.5.0)\r\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\r\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\r\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: jinja2 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\r\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\r\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community)\r\n",
      "  Using cached mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/axel/Documents/Cours_Mosef/Embeddings/venv/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\r\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m10.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading openai-1.109.0-py3-none-any.whl (948 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m948.4/948.4 kB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading datasets-4.1.1-py3-none-any.whl (503 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m503.6/503.6 kB\u001B[0m \u001B[31m9.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m11.6/11.6 MB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m11.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.0/75.0 kB\u001B[0m \u001B[31m6.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\r\n",
      "Downloading faiss_cpu-1.12.0-cp39-cp39-macosx_14_0_arm64.whl (3.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.4/3.4 MB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m486.6/486.6 kB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading aiohttp-3.12.15-cp39-cp39-macosx_11_0_arm64.whl (469 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m469.2/469.2 kB\u001B[0m \u001B[31m6.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\r\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\r\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m119.7/119.7 kB\u001B[0m \u001B[31m10.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached distro-1.9.0-py3-none-any.whl (20 kB)\r\n",
      "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m199.3/199.3 kB\u001B[0m \u001B[31m8.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\r\n",
      "Downloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m563.3/563.3 kB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading jiter-0.11.0-cp39-cp39-macosx_11_0_arm64.whl (305 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m305.9/305.9 kB\u001B[0m \u001B[31m13.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m447.5/447.5 kB\u001B[0m \u001B[31m8.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading langsmith-0.4.30-py3-none-any.whl (386 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m386.3/386.3 kB\u001B[0m \u001B[31m12.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.4/133.4 kB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hUsing cached pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl (31.2 MB)\r\n",
      "Downloading pydantic-2.11.9-py3-none-any.whl (444 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m444.9/444.9 kB\u001B[0m \u001B[31m9.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached pydantic_core-2.33.2-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\r\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.2/45.2 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m432.2/432.2 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading sqlalchemy-2.0.43-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hUsing cached tenacity-9.1.2-py3-none-any.whl (28 kB)\r\n",
      "Downloading tiktoken-0.11.0-cp39-cp39-macosx_11_0_arm64.whl (1.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/1.0 MB\u001B[0m \u001B[31m7.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.9/2.9 MB\u001B[0m \u001B[31m7.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading torch-2.8.0-cp39-none-macosx_11_0_arm64.whl (73.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.6/73.6 MB\u001B[0m \u001B[31m8.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading filelock-3.19.1-py3-none-any.whl (15 kB)\r\n",
      "Using cached xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\r\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\r\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\r\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\n",
      "Downloading frozenlist-1.7.0-cp39-cp39-macosx_11_0_arm64.whl (47 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m47.2/47.2 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.6/2.6 MB\u001B[0m \u001B[31m11.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\r\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\r\n",
      "Downloading multidict-6.6.4-cp39-cp39-macosx_11_0_arm64.whl (44 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m44.5/44.5 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading orjson-3.11.3-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (238 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m238.2/238.2 kB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading propcache-0.3.2-cp39-cp39-macosx_11_0_arm64.whl (43 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.7/43.7 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\r\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\r\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.3/6.3 MB\u001B[0m \u001B[31m11.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\r\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading yarl-1.20.1-cp39-cp39-macosx_11_0_arm64.whl (89 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m90.0/90.0 kB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading zstandard-0.25.0-cp39-cp39-macosx_11_0_arm64.whl (640 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m640.6/640.6 kB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\r\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "Using cached mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\r\n",
      "Installing collected packages: mpmath, zstandard, xxhash, typing-inspection, tenacity, sympy, SQLAlchemy, safetensors, python-dotenv, pydantic-core, pyarrow, propcache, orjson, networkx, mypy-extensions, multidict, marshmallow, jsonpatch, jiter, httpx-sse, hf-xet, fsspec, frozenlist, filelock, faiss-cpu, distro, dill, async-timeout, annotated-types, aiohappyeyeballs, yarl, typing-inspect, torch, tiktoken, requests-toolbelt, pydantic, multiprocess, huggingface-hub, aiosignal, tokenizers, pydantic-settings, dataclasses-json, aiohttp, transformers, openai, langsmith, sentence-transformers, langchain-core, datasets, langchain_text_splitters, langchain_openai, langchain, langchain_community\r\n",
      "Successfully installed SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 async-timeout-4.0.3 dataclasses-json-0.6.7 datasets-4.1.1 dill-0.4.0 distro-1.9.0 faiss-cpu-1.12.0 filelock-3.19.1 frozenlist-1.7.0 fsspec-2025.9.0 hf-xet-1.1.10 httpx-sse-0.4.1 huggingface-hub-0.35.1 jiter-0.11.0 jsonpatch-1.33 langchain-0.3.27 langchain-core-0.3.76 langchain_community-0.3.29 langchain_openai-0.3.33 langchain_text_splitters-0.3.11 langsmith-0.4.30 marshmallow-3.26.1 mpmath-1.3.0 multidict-6.6.4 multiprocess-0.70.16 mypy-extensions-1.1.0 networkx-3.2.1 openai-1.109.0 orjson-3.11.3 propcache-0.3.2 pyarrow-21.0.0 pydantic-2.11.9 pydantic-core-2.33.2 pydantic-settings-2.10.1 python-dotenv-1.1.1 requests-toolbelt-1.0.0 safetensors-0.6.2 sentence-transformers-5.1.1 sympy-1.14.0 tenacity-9.1.2 tiktoken-0.11.0 tokenizers-0.22.1 torch-2.8.0 transformers-4.56.2 typing-inspect-0.9.0 typing-inspection-0.4.1 xxhash-3.5.0 yarl-1.20.1 zstandard-0.25.0\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.3.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai datasets transformers PyMuPDF langchain_community langchain_openai langchain_text_splitters faiss-cpu sentence-transformers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-23T17:03:12.500933Z",
     "start_time": "2025-09-23T17:02:28.768969Z"
    }
   },
   "id": "61d8f9f142d9ceb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM\n",
    "### OpenAI\n",
    "**Objectif** : faire une requête au modèle 'gpt4o-mini' via la librairie d'OpenAI et récupérer : \n",
    "- la réponse \n",
    "- le nombre de tokens d'entrée\n",
    "- le nombre de tokens de sorties"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d1105871b990623"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T09:36:16.034922Z",
     "start_time": "2024-11-22T09:36:15.401270Z"
    }
   },
   "id": "31fe75b73730df71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ask_gpt(question: str):\n",
    "    \"\"\"\n",
    "    Prend une question et retourne la réponse ainsi que les KPIs d'usage\n",
    "    \n",
    "    Args: \n",
    "        question(str) : question à poser\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "answer, usage = ask_gpt(\"Qu'est ce qu'un LLM ?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c46fdaaf0bf14b59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAGS from Scracth\n",
    "**Objectif** : Développer, from scratch, une architecture RAG en 4 modules: \n",
    "- Lecture et découpage d'un document PDF\n",
    "- Générer les embeddings\n",
    "- Récupérer les passages pertinents en fonction d'une requête\n",
    "- Générer une réponse à partir de la question et des passages pertinents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef6c3a1a644c5bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3030e0ae2cae73b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Module 1 : Lecture et découpage d’un document PDF\n",
    "def read_and_split_pdf(pdf_path, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Lit un fichier PDF et découpe le contenu en morceaux de texte.\n",
    "    \n",
    "    Args:\n",
    "    pdf_path (str): Chemin du fichier PDF.\n",
    "    chunk_size (int): Taille des morceaux (en nombre de caractères).\n",
    "    \n",
    "    Returns:\n",
    "    list: Liste des morceaux de texte.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Tester avec un fichier PDF\n",
    "pdf_chunks = read_and_split_pdf(\"../pdf/Cours_LLM.pdf\")\n",
    "print(f\"Nombre de morceaux générés : {len(pdf_chunks)}\")\n",
    "print(\"Extrait du premier morceau :\\n\", pdf_chunks[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b6027e8261adc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Module 2 : Génération des embeddings et stockage avec FAISS\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Génère les embeddings pour chaque morceau de texte.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): Liste des morceaux de texte.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Tableau numpy des embeddings.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Crée un index FAISS à partir des embeddings.\n",
    "    \n",
    "    Args:\n",
    "    embeddings (np.array): Tableau numpy des embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    faiss.IndexFlatL2: Index FAISS.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Générer les embeddings et créer l'index\n",
    "embeddings = generate_embeddings(pdf_chunks)\n",
    "faiss_index = create_faiss_index(embeddings)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46f8d025d6d0836"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Module 3 : Recherche des passages pertinents avec une requête\n",
    "def query_faiss_index(query, index, model, chunks, k=5):\n",
    "    \"\"\"\n",
    "    Recherche les passages les plus pertinents dans l'index FAISS.\n",
    "    \n",
    "    Args:\n",
    "    query (str): La requête de l'utilisateur.\n",
    "    index (faiss.IndexFlatL2): L'index FAISS.\n",
    "    model (SentenceTransformer): Modèle pour générer les embeddings.\n",
    "    chunks (list): Liste des morceaux de texte.\n",
    "    k (int): Nombre de passages à retourner.\n",
    "    \n",
    "    Returns:\n",
    "    list: Liste des passages pertinents.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Tester avec une requête\n",
    "query = \"Qu'est ce qu'un RAG ?\"\n",
    "relevant_chunks = query_faiss_index(query, faiss_index, SentenceTransformer('all-MiniLM-L6-v2'), pdf_chunks)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50536e72bf4ba48a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Module 4 : Générer une réponse avec un LLM\n",
    "def generate_answer(query, relevant_chunks):\n",
    "    \"\"\"\n",
    "    Génère une réponse à partir de la question et des passages pertinents.\n",
    "    \n",
    "    Args:\n",
    "    query (str): La question posée.\n",
    "    relevant_chunks (list): Passages pertinents.\n",
    "    \n",
    "    Returns:\n",
    "    str: Réponse générée.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Tester la génération de réponse\n",
    "response = generate_answer(query, relevant_chunks)\n",
    "print(\"Réponse générée :\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e702d0b4e58f115"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Langchain\n",
    "**Objectif** : faire une requête au modèle 'gpt4o-mini' via la librairie Langchain et récupérer la réponse "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b967dd1bc6dce1cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60ab86e73bae2fec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt_langchain(question):\n",
    "    \"\"\"\n",
    "    Prend une question et retourne la réponse générée via langchain\n",
    "    \n",
    "    Args: \n",
    "        question(str) : question à poser\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "anwser = ask_gpt_langchain(\"Qu'est ce qu'un transformer ?\")\n",
    "print(anwser)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lecture d'un PDF et utilisation dans RAG avec LangChain\n",
    "**Objectif** : Construire une architecture RAG avec langchain via 3 modules : \n",
    "1 - récupération du document via PyPDFLoader\n",
    "2 - génération du retriever après avoir split le document\n",
    "3 - Création de la chaîne de réponse à la question\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bf6056"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import OpenAIEmbeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da9a3d3886988a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_docs(pdf_path):\n",
    "    \"\"\"\n",
    "    Charge et extrait le contenu textuel d'un fichier PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Chemin vers le fichier PDF à charger.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste contenant le contenu textuel de chaque page du PDF.\n",
    "    \"\"\"\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fef1ac6bfe8ca792"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_retriever(docs):\n",
    "    \"\"\"\n",
    "    Crée un récupérateur (retriever) basé sur les embeddings pour une recherche de similarité.\n",
    "\n",
    "    Args:\n",
    "        docs (list): Liste des documents à traiter\n",
    "\n",
    "    Returns:\n",
    "        BaseRetriever\n",
    "    \"\"\"\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cc4c3cf808a19f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_answer(question, retriever) :\n",
    "    \"\"\"\n",
    "    Génère une réponse à une question en utilisant une architecture RAG.\n",
    "\n",
    "    Args:\n",
    "        question (str): La question posée.\n",
    "        retriever (BaseRetriever): Un récupérateur permettant de chercher le contexte pertinent\n",
    "                                   pour répondre à la question.\n",
    "\n",
    "    Returns:\n",
    "        str: La réponse générée par le modèle, basée sur le contexte récupéré. \n",
    "    \"\"\"\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7640393926e24d78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs = get_docs('../pdf/Cours_LLM.pdf')\n",
    "retriever = get_retriever(docs)\n",
    "results = generate_answer(\"Qu'est ce qu'un RAG ?\", retriever)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3c509ef9aeb7a02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca343056a836af6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"context\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fee632557fc2dd98"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
