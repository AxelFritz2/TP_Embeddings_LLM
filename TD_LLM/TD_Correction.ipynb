{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ff10c4",
   "metadata": {},
   "source": [
    "# TP : LLM, Langchain et RAG\n",
    "## Installation des dépendances"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prérequis : \n",
    "- Mettre la clé d'API OpenAI en variable d'environnement\n",
    "- Installer les packages suivants : "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e053c2e3959006e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install langchain openai datasets transformers PyMuPDF langchain_community langchain_openai langchain_text_splitters faiss-cpu sentence-transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61d8f9f142d9ceb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM\n",
    "### OpenAI\n",
    "**Objectif** : faire une requête au modèle 'gpt4o-mini' via la librairie d'OpenAI et récupérer : \n",
    "- la réponse \n",
    "- le nombre de tokens d'entrée\n",
    "- le nombre de tokens de sorties"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d1105871b990623"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-23T17:04:05.864147Z",
     "start_time": "2025-09-23T17:04:04.903359Z"
    }
   },
   "id": "31fe75b73730df71"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def ask_gpt(question: str):\n",
    "    \"\"\"\n",
    "    Prend une question et retourne la réponse ainsi que les KPIs d'usage\n",
    "    \n",
    "    Args: \n",
    "        question(str) : question à poser\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return(completion.choices[0].message.content, completion.usage)\n",
    "\n",
    "answer, usage = ask_gpt(\"Qu'est ce qu'un LLM ?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:31:12.202483Z",
     "start_time": "2024-11-22T11:31:08.314038Z"
    }
   },
   "id": "c46fdaaf0bf14b59"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "CompletionUsage(completion_tokens=239, prompt_tokens=25, total_tokens=264, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:31:14.936796Z",
     "start_time": "2024-11-22T11:31:14.919123Z"
    }
   },
   "id": "8443bd79c414fa37"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "'Un LLM, ou \"Large Language Model\" (modèle de langage de grande taille en français), est un type de modèle d\\'intelligence artificielle conçu pour comprendre et générer du texte en langage naturel. Ces modèles sont entraînés sur d\\'énormes ensembles de données textuelles provenant de diverses sources, ce qui leur permet d\\'apprendre les structures des langues, le vocabulaire, et même le contexte dans lequel les mots se rejoignent.\\n\\nLes LLM fonctionnent généralement grâce à des architectures de type réseau de neurones, tels que les transformateurs, qui leur permettent de traiter des séquences de texte et de créer des prédictions sur le mot suivant dans une phrase, entre autres tâches.\\n\\nLes applications des LLM sont variées, allant de la génération de texte (comme écrire des articles ou des histoires) à la traduction automatique, en passant par la conversation et l\\'assistance dans diverses tâches linguistiques.\\n\\nDes exemples connus de LLM incluent GPT-3 et GPT-4 d\\'OpenAI, BERT de Google, et T5, parmi d\\'autres. Ils ont démontré des capacités impressionnantes dans de nombreuses tâches de traitement du langage naturel.'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:31:20.396395Z",
     "start_time": "2024-11-22T11:31:20.383035Z"
    }
   },
   "id": "e460230c5d2b47c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAGS from Scracth\n",
    "**Objectif** : Développer, from scratch, une architecture RAG en 4 modules: \n",
    "- Lecture et découpage d'un document PDF\n",
    "- Générer les embeddings\n",
    "- Récupérer les passages pertinents en fonction d'une requête\n",
    "- Générer une réponse à partir de la question et des passages pertinents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef6c3a1a644c5bc"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axel/Documents/Nexialog/Formation LLM/TP/venv/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:33:00.261836Z",
     "start_time": "2024-11-22T11:32:45.112616Z"
    }
   },
   "id": "3030e0ae2cae73b1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de morceaux générés : 87\n",
      "Extrait du premier morceau :\n",
      " Large Language Models\n",
      "Introduction\n",
      "Les modèles de langage de grande taille (LLM) représentent une avancée majeure dans le domaine\n",
      "de l’intelligence artificielle, avec des applications variées allant de la génération de texte à l’analyse\n",
      "sémantique et à la traduction automatique. En s’appuyant sur des architectures de réseaux neuronaux,\n",
      "notamment les modèles Transformers, les LLM permettent de traiter, comprendre et générer du langage\n",
      "naturel de manière performante et cohérente.\n",
      "Nous explorerons \n"
     ]
    }
   ],
   "source": [
    "# Module 1 : Lecture et découpage d’un document PDF\n",
    "def read_and_split_pdf(pdf_path, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Lit un fichier PDF et découpe le contenu en morceaux de texte.\n",
    "    \n",
    "    Args:\n",
    "    pdf_path (str): Chemin du fichier PDF.\n",
    "    chunk_size (int): Taille des morceaux (en nombre de caractères).\n",
    "    \n",
    "    Returns:\n",
    "    list: Liste des morceaux de texte.\n",
    "    \"\"\"\n",
    "    text_chunks = []\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text()\n",
    "        text_chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "    return text_chunks\n",
    "\n",
    "# Tester avec un fichier PDF\n",
    "pdf_chunks = read_and_split_pdf(\"../pdf/Cours_LLM.pdf\")\n",
    "print(f\"Nombre de morceaux générés : {len(pdf_chunks)}\")\n",
    "print(\"Extrait du premier morceau :\\n\", pdf_chunks[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:33:00.637875Z",
     "start_time": "2024-11-22T11:33:00.264983Z"
    }
   },
   "id": "54b6027e8261adc"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d3c5934d9654fd587469585064d43c3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de vecteurs dans l'index : 87\n"
     ]
    }
   ],
   "source": [
    "# Module 2 : Génération des embeddings et stockage avec FAISS\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Génère les embeddings pour chaque morceau de texte.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): Liste des morceaux de texte.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Tableau numpy des embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Crée un index FAISS à partir des embeddings.\n",
    "    \n",
    "    Args:\n",
    "    embeddings (np.array): Tableau numpy des embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    faiss.IndexFlatL2: Index FAISS.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Générer les embeddings et créer l'index\n",
    "embeddings = generate_embeddings(pdf_chunks)\n",
    "faiss_index = create_faiss_index(embeddings)\n",
    "\n",
    "print(f\"Nombre de vecteurs dans l'index : {faiss_index.ntotal}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:35:19.671439Z",
     "start_time": "2024-11-22T11:35:15.566461Z"
    }
   },
   "id": "46f8d025d6d0836"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passages pertinents :\n",
      "--------------------------------------------------------------------------------\n",
      "Génération de la base de données : Pour créer la base de données sur laquelle le RAG va\n",
      "s’appuyer, on transforme l’ensemble des documents par le biais d’embeddings. On représente\n",
      "l’ensemble des paragraphes sous forme de vecteurs, ce qui permet de capturer des relations sé-\n",
      "mantiques entre les mots et de les rendre compréhensibles par la machine.\n",
      "•Récupération d’informations : Lorsqu’une requête est reçue, le RAG commence par identifier\n",
      "les segments de texte pertinents dans sa base de données. Le\n",
      "--------------------------------------------------------------------------------\n",
      "cifiques récupérées. Cela permet de produire des réponses qui sont à la fois informatives\n",
      "et particulièrement adaptées à la requête.\n",
      "5.2 Avantages\n",
      "L’utilisation de RAG offre plusieurs avantages significatifs :\n",
      "•Amélioration de l’exactitude : En s’appuyant sur des informations précises récupérées, les\n",
      "RAG peuvent fournir des réponses plus exactes que les LLM standards, surtout dans des domaines\n",
      "nécessitant une expertise spécifique. Cela permet de réduire les problèmes d’hallucination, où le\n",
      "modèl\n",
      "--------------------------------------------------------------------------------\n",
      " ensemble,\n",
      "en portant une attention particulière aux relations entre tous les mots de la séquence.\n",
      "LeprincipecléderrièrelesTransformersestcequ’onappellelemécanismed’attention. Cemécanisme\n",
      "permet au modèle de se concentrer sur des mots spécifiques dans une phrase, en fonction du contexte, et\n",
      "d’ignorer ceux qui sont moins pertinents pour une tâche donnée. Par exemple, dans la phrase \"Le chat\n",
      "noir saute par-dessus le mur\", le Transformer peut déterminer que \"chat\" et \"saute\" sont fortement liés,\n",
      "ca\n",
      "--------------------------------------------------------------------------------\n",
      "nformations contextuelles spécifiques provenant de la\n",
      "récupération de documents, les RAG peuvent générer des réponses qui sont non seulement correctes\n",
      "mais aussi riches en contenu et en détail. Cela est particulièrement utile pour des applications telles\n",
      "que les outils de recherche spécialisée.\n",
      "•Réduction des coûts d’entraînement : Puisque les RAG peuvent être mis à jour simplement\n",
      "en ajoutant ou en modifiant les documents dans leur base de données de récupération, ils réduisent\n",
      "les besoins en r\n",
      "--------------------------------------------------------------------------------\n",
      "s Q(i),K(i), etV(i)pour chaque entrée. On obtient donc yétats cachés par entrée.\n",
      "Pour obtenir l’état caché hde chaque entrée i, on va devoir concaténer les h1\n",
      "i, ..., hy\n",
      "iétats cachés en un\n",
      "état noté Zdans le schéma suivant.\n",
      "Figure 4: Têtes d’Attention Multiples\n",
      "123.2.2 Extension verticale : Ajout de couches\n",
      "Après avoir calculé les états cachés des différentes têtes et les avoir concaténés, on ne va plus prendre\n",
      "l’état caché obtenu comme état final. Le vecteur concaténé va ainsi passer dans une \n"
     ]
    }
   ],
   "source": [
    "# Module 3 : Recherche des passages pertinents avec une requête\n",
    "def query_faiss_index(query, index, model, chunks, k=5):\n",
    "    \"\"\"\n",
    "    Recherche les passages les plus pertinents dans l'index FAISS.\n",
    "    \n",
    "    Args:\n",
    "    query (str): La requête de l'utilisateur.\n",
    "    index (faiss.IndexFlatL2): L'index FAISS.\n",
    "    model (SentenceTransformer): Modèle pour générer les embeddings.\n",
    "    chunks (list): Liste des morceaux de texte.\n",
    "    k (int): Nombre de passages à retourner.\n",
    "    \n",
    "    Returns:\n",
    "    list: Liste des passages pertinents.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# Tester avec une requête\n",
    "query = \"Qu'est ce qu'un RAG ?\"\n",
    "relevant_chunks = query_faiss_index(query, faiss_index, SentenceTransformer('all-MiniLM-L6-v2'), pdf_chunks)\n",
    "print(\"Passages pertinents :\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(\"-\" * 80)\n",
    "    print(chunk)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:36:31.667357Z",
     "start_time": "2024-11-22T11:36:29.723834Z"
    }
   },
   "id": "50536e72bf4ba48a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse générée :\n",
      "Un RAG, ou \"Retrieval-Augmented Generation\", est un modèle de traitement du langage qui combine des capacités de récupération d'informations avec des mécanismes de génération de texte. Il fonctionne en deux étapes principales :\n",
      "\n",
      "1. **Génération de la base de données** : Les documents pertinents sont transformés en vecteurs par le biais d'embeddings, ce qui permet de capturer les relations sémantiques entre les mots. Cela facilite la compréhension du texte par la machine.\n",
      "\n",
      "2. **Récupération d'informations** : Lorsqu'une requête est formulée, le RAG identifie les segments de texte pertinents dans sa base de données. Il utilise ces informations contextuelles spécifiques pour générer des réponses informatives et adaptées au contexte de la question posée.\n",
      "\n",
      "Les avantages des RAG incluent une amélioration de l'exactitude des réponses grâce à l'utilisation d'informations précises, une réduction des problèmes d'hallucination par rapport aux modèles de langage standard, et une capacité d'adaptation accrue en permettant des mises à jour simples de la base de données.\n"
     ]
    }
   ],
   "source": [
    "# Module 4 : Générer une réponse avec un LLM\n",
    "def generate_answer(query, relevant_chunks):\n",
    "    \"\"\"\n",
    "    Génère une réponse à partir de la question et des passages pertinents.\n",
    "    \n",
    "    Args:\n",
    "    query (str): La question posée.\n",
    "    relevant_chunks (list): Passages pertinents.\n",
    "    \n",
    "    Returns:\n",
    "    str: Réponse générée.\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    prompt = f\"Voici des informations utiles :\\n{context}\\n\\nQuestion : {query}\\nRéponse :\"\n",
    "    \n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Tu es un assistant RAG spécialisé dans la rédaction de réponse.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return(completion.choices[0].message.content)\n",
    "\n",
    "# Tester la génération de réponse\n",
    "response = generate_answer(query, relevant_chunks)\n",
    "print(\"Réponse générée :\")\n",
    "print(response)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:37:04.767818Z",
     "start_time": "2024-11-22T11:37:00.991250Z"
    }
   },
   "id": "9e702d0b4e58f115"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Langchain\n",
    "**Objectif** : faire une requête au modèle 'gpt4o-mini' via la librairie Langchain et récupérer la réponse "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b967dd1bc6dce1cc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:38:42.437468Z",
     "start_time": "2024-11-22T11:38:41.438054Z"
    }
   },
   "id": "60ab86e73bae2fec"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d14d2705",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T11:38:47.829951Z",
     "start_time": "2024-11-22T11:38:42.439977Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/12/33kq22951gn7jfpz3d32v_qc0000gn/T/ipykernel_4641/2562852100.py:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Un transformer est un modèle d\\'apprentissage automatique utilisé principalement dans le domaine du traitement du langage naturel (NLP). Introduit dans l\\'article \"Attention is All You Need\" par Vaswani et al. en 2017, le transformer a révolutionné la manière dont les modèles sont construits pour traiter des séquences de données.\\n\\nVoici quelques caractéristiques clés des transformers :\\n\\n1. **Architecture basée sur l\\'attention** : Contrairement aux modèles précédents, comme les réseaux de neurones récurrents (RNN), qui traitent les données de manière séquentielle, les transformers utilisent un mécanisme d\\'attention qui permet de traiter toutes les entrées simultanément. Cela permet de capturer les relations entre les mots, peu importe leur position dans la séquence.\\n\\n2. **Encoders et Decoders** : L\\'architecture d\\'un transformer est généralement divisée en deux parties : l\\'encodeur, qui transforme la séquence d\\'entrée en une représentation interne, et le décodeur, qui génère la séquence de sortie à partir de cette représentation. Cependant, certains modèles, comme BERT, n\\'utilisent que l\\'encodeur, tandis que d\\'autres, comme GPT, n\\'utilisent que le décodeur.\\n\\n3. **Parallélisation** : Grâce à son architecture, le transformer permet une parallélisation plus efficace lors de l\\'entraînement, ce qui le rend particulièrement adapté aux grandes quantités de données.\\n\\n4. **Applications variées** : Bien qu\\'initialement conçu pour le traitement du langage, le modèle transformer a été adapté à d\\'autres tâches, comme la vision par ordinateur, la musique, et même la biologie.\\n\\n5. **Modèles dérivés** : Depuis l\\'introduction du transformer, de nombreux modèles ont été développés sur cette base, tels que BERT, GPT, T5, et bien d\\'autres, chacun ayant des spécificités et des applications particulières.\\n\\nEn résumé, les transformers sont des modèles puissants qui ont largement amélioré les performances dans de nombreuses tâches de traitement des données, en particulier dans le domaine du langage naturel.'}\n"
     ]
    }
   ],
   "source": [
    "def ask_gpt_langchain(question):\n",
    "    \"\"\"\n",
    "    Prend une question et retourne la réponse générée via langchain\n",
    "    \n",
    "    Args: \n",
    "        question(str) : question à poser\n",
    "    \"\"\"\n",
    "        \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    prompt_template = PromptTemplate.from_template(question)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    \n",
    "    return(chain.invoke({}))\n",
    "\n",
    "anwser = ask_gpt_langchain(\"Qu'est ce qu'un transformer ?\")\n",
    "print(anwser)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lecture d'un PDF et utilisation dans RAG avec LangChain\n",
    "**Objectif** : Construire une architecture RAG avec langchain via 3 modules : \n",
    "1 - récupération du document via PyPDFLoader\n",
    "2 - génération du retriever après avoir split le document\n",
    "3 - Création de la chaîne de réponse à la question\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bf6056"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import OpenAIEmbeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:39:04.508332Z",
     "start_time": "2024-11-22T11:39:04.461304Z"
    }
   },
   "id": "4da9a3d3886988a9"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_docs(pdf_path):\n",
    "    \"\"\"\n",
    "    Charge et extrait le contenu textuel d'un fichier PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Chemin vers le fichier PDF à charger.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste contenant le contenu textuel de chaque page du PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    return(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:39:04.776417Z",
     "start_time": "2024-11-22T11:39:04.765770Z"
    }
   },
   "id": "fef1ac6bfe8ca792"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def get_retriever(docs):\n",
    "    \"\"\"\n",
    "    Crée un récupérateur (retriever) basé sur les embeddings pour une recherche de similarité.\n",
    "\n",
    "    Args:\n",
    "        docs (list): Liste des documents à traiter\n",
    "\n",
    "    Returns:\n",
    "        BaseRetriever\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, \n",
    "                                                   chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    vectorstore = InMemoryVectorStore.from_documents(documents=splits, \n",
    "                                                     embedding=OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:40:58.430373Z",
     "start_time": "2024-11-22T11:40:58.419728Z"
    }
   },
   "id": "5cc4c3cf808a19f3"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def generate_answer(question, retriever) :\n",
    "    \"\"\"\n",
    "    Génère une réponse à une question en utilisant une architecture RAG.\n",
    "\n",
    "    Args:\n",
    "        question (str): La question posée.\n",
    "        retriever (BaseRetriever): Un récupérateur permettant de chercher le contexte pertinent\n",
    "                                   pour répondre à la question.\n",
    "\n",
    "    Returns:\n",
    "        str: La réponse générée par le modèle, basée sur le contexte récupéré. \n",
    "    \"\"\"\n",
    "    system_prompt = (\"You are an assistant for question-answering tasks. \"\n",
    "                     \"Use the following pieces of retrieved context to answer \"\n",
    "                     \"the question. If you don't know the answer, say that you \"\n",
    "                     \"don't know. Use three sentences maximum and keep the \"\n",
    "                     \"answer concise.\"\n",
    "                     \"\\n\\n\"\n",
    "                     \"{context}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt),\n",
    "                                               (\"human\", \"{input}\"),])\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    \n",
    "    results = rag_chain.invoke({\"input\": f\"{question}\"})\n",
    "    \n",
    "    return(results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:40:58.769586Z",
     "start_time": "2024-11-22T11:40:58.761709Z"
    }
   },
   "id": "7640393926e24d78"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "docs = get_docs('../pdf/Cours_LLM.pdf')\n",
    "retriever = get_retriever(docs)\n",
    "results = generate_answer(\"Qu'est ce qu'un RAG ?\", retriever)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:41:10.434192Z",
     "start_time": "2024-11-22T11:40:59.024128Z"
    }
   },
   "id": "f3c509ef9aeb7a02"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un RAG (Retrieval Augmented Generation) est un système qui combine la capacité de génération d'un modèle de langage (LLM) avec un mécanisme de récupération d'informations. Cela permet d'améliorer la qualité et la précision des réponses en utilisant des données spécifiques d'une vaste base de données. Les RAG sont également plus flexibles et réduisent les coûts d'entraînement en permettant des mises à jour sans réentraînement complet du modèle.\n"
     ]
    }
   ],
   "source": [
    "print(results[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:41:10.447774Z",
     "start_time": "2024-11-22T11:41:10.436181Z"
    }
   },
   "id": "ca343056a836af6d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='66d6b942-8604-4319-a1e0-84b775bb9ead', metadata={'source': '../pdf/Cours_LLM.pdf', 'page': 18}, page_content='5 RAG : Retrieval Augmented Generation\\nLes systèmes de génération augmentée par récupération d’informations (RAG) sont une évolution\\ndes LLM. Un RAG combine la capacité de génération d’un LLM avec un mécanisme de récupération\\nd’informations pour améliorer la qualité et la précision des réponses du modèle. Ce mécanisme permet\\nau modèle de rechercher et d’utiliser des informations spécifiques stockées dans une vaste base de données'), Document(id='a3f30ddd-1225-4cbe-97ff-60fdb8b4dfc2', metadata={'source': '../pdf/Cours_LLM.pdf', 'page': 19}, page_content='que les outils de recherche spécialisée.\\n•Réduction des coûts d’entraînement : Puisque les RAG peuvent être mis à jour simplement\\nen ajoutant ou en modifiant les documents dans leur base de données de récupération, ils réduisent\\nles besoins en réentraînement complet du modèle de base. Cela conduit à une réduction des coûts\\ncomputationnels et de temps associés à l’entraînement de grands modèles de langage.\\nFigure 10: Schéma Architecture RAG\\n20'), Document(id='81c361b3-e383-43e9-a53c-121e5aacf196', metadata={'source': '../pdf/Cours_LLM.pdf', 'page': 18}, page_content='données spécifiques récupérées. Cela permet de produire des réponses qui sont à la fois informatives\\net particulièrement adaptées à la requête.\\n5.2 Avantages\\nL’utilisation de RAG offre plusieurs avantages significatifs :\\n•Amélioration de l’exactitude : En s’appuyant sur des informations précises récupérées, les\\nRAG peuvent fournir des réponses plus exactes que les LLM standards, surtout dans des domaines'), Document(id='7ce70016-aa4e-4093-94c3-2ee242ff840a', metadata={'source': '../pdf/Cours_LLM.pdf', 'page': 18}, page_content='•Amélioration de l’exactitude : En s’appuyant sur des informations précises récupérées, les\\nRAG peuvent fournir des réponses plus exactes que les LLM standards, surtout dans des domaines\\nnécessitant une expertise spécifique. Cela permet de réduire les problèmes d’hallucination, où le\\nmodèle génère des informations incorrectes ou inventées.\\n•Flexibilité et adaptation : Les RAG peuvent s’adapter à de nouvelles informations et domaines')]\n"
     ]
    }
   ],
   "source": [
    "print(results[\"context\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:41:10.450663Z",
     "start_time": "2024-11-22T11:41:10.442429Z"
    }
   },
   "id": "fee632557fc2dd98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abcbaef07a026085"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data Science is a multidisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It's an amalgamation of Statistics, Computer Science, Mathematics, and Domain Expertise. The goal is to make sense of the data, understand patterns, make predictions or decisions, and support decision-making processes through data analysis and interpretation.\n",
      "Data Science involves several key stages:\n",
      "1. **Data Collection**: This is the first step where raw data is gathered from various sources such as databases, files, APIs, and real-time data streams.\n",
      "2. **Data Cleaning**: The collected data needs to be cleaned and preprocessed to handle missing values, outliers, inconsistencies, and other issues that can affect the quality of analysis.\n",
      "3. **Data Exploration**: This step involves understanding the data by exploring its properties, generating descriptive statistics, visualizing it, and discovering initial patterns or insights.\n",
      "4. **Model Building**: Here, statistical models, machine learning algorithms, or predictive modeling techniques are applied to the data to make predictions, classify items, identify trends, or extract knowledge.\n",
      "5. **Evaluation**: The model's performance is assessed using various metrics and the results are validated against a test dataset. If the model performs well, it can be deployed for real-world applications.\n",
      "6. **Deployment and Monitoring**: The final step involves deploying the model into production, where it can be used to make predictions, inform decisions, or automate processes. After deployment, the model's performance should be continuously monitored and adjusted as necessary.\n",
      "Data Science is not just about technical skills; it also requires good communication, problem-solving abilities, and business acumen. With the increasing availability of data and the ever-growing need for insights, Data Science has become a critical skill set in various industries such as finance, healthcare, marketing, and more."
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='mistral', messages=[\n",
    "   {\n",
    "     'role': 'user',\n",
    "     'content': 'Talk about Data Science',\n",
    "   },\n",
    " ], stream=True)\n",
    "\n",
    "\"for chunk in response:    print(chunk[\"message\"][\"content\"], end='', flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:44:28.079524Z",
     "start_time": "2024-11-22T11:43:24.576192Z"
    }
   },
   "id": "829de9b16e1c52bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
