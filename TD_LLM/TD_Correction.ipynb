{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ff10c4",
   "metadata": {},
   "source": [
    "# TP : LLM, Langchain et RAG\n",
    "## Installation des dépendances"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prérequis : \n",
    "- Mettre la clé d'API OpenAI en variable d'environnement\n",
    "- Installer les packages suivants : "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e053c2e3959006e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install langchain openai datasets transformers PyMuPDF langchain_community langchain_openai langchain_text_splitters faiss-cpu sentence-transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61d8f9f142d9ceb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LLM\n",
    "### OpenAI\n",
    "**Objectif** : faire une requête au modèle 'gpt4o-mini' via la librairie d'OpenAI et récupérer : \n",
    "- la réponse \n",
    "- le nombre de tokens d'entrée\n",
    "- le nombre de tokens de sorties"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d1105871b990623"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31fe75b73730df71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ask_gpt(question: str):\n",
    "    \"\"\"\n",
    "    Prend une question et retourne la réponse ainsi que les KPIs d'usage\n",
    "    \n",
    "    Args: \n",
    "        question(str) : question à poser\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return(completion.choices[0].message.content, completion.usage)\n",
    "\n",
    "answer, usage = ask_gpt(\"Qu'est ce qu'un LLM ?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c46fdaaf0bf14b59"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "usage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8443bd79c414fa37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "answer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e460230c5d2b47c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RAGS from Scracth\n",
    "**Objectif** : Développer, from scratch, une architecture RAG en 4 modules: \n",
    "- Lecture et découpage d'un document PDF\n",
    "- Générer les embeddings\n",
    "- Récupérer les passages pertinents en fonction d'une requête\n",
    "- Générer une réponse à partir de la question et des passages pertinents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ef6c3a1a644c5bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3030e0ae2cae73b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Module 1 : Lecture et découpage d’un document PDF\n",
    "def read_and_split_pdf(pdf_path, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Lit un fichier PDF et découpe le contenu en morceaux de texte.\n",
    "    \n",
    "    Args:\n",
    "    pdf_path (str): Chemin du fichier PDF.\n",
    "    chunk_size (int): Taille des morceaux (en nombre de caractères).\n",
    "    \n",
    "    Returns:\n",
    "    list: Liste des morceaux de texte.\n",
    "    \"\"\"\n",
    "    text_chunks = []\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        full_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            full_text += page.extract_text()\n",
    "        text_chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]\n",
    "    return text_chunks\n",
    "\n",
    "# Tester avec un fichier PDF\n",
    "pdf_chunks = read_and_split_pdf(\"../Data/Cours_LLM.pdf\")\n",
    "print(f\"Nombre de morceaux générés : {len(pdf_chunks)}\")\n",
    "print(\"Extrait du premier morceau :\\n\", pdf_chunks[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54b6027e8261adc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Module 2 : Génération des embeddings et stockage avec FAISS\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Génère les embeddings pour chaque morceau de texte.\n",
    "    \n",
    "    Args:\n",
    "        chunks (list): Liste des morceaux de texte.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Tableau numpy des embeddings.\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Crée un index FAISS à partir des embeddings.\n",
    "    \n",
    "    Args:\n",
    "    embeddings (np.array): Tableau numpy des embeddings.\n",
    "    \n",
    "    Returns:\n",
    "    faiss.IndexFlatL2: Index FAISS.\n",
    "    \"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# Générer les embeddings et créer l'index\n",
    "embeddings = generate_embeddings(pdf_chunks)\n",
    "faiss_index = create_faiss_index(embeddings)\n",
    "\n",
    "print(f\"Nombre de vecteurs dans l'index : {faiss_index.ntotal}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46f8d025d6d0836"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Module 3 : Recherche des passages pertinents avec une requête\n",
    "def query_faiss_index(query, index, model, chunks, k=5):\n",
    "    \"\"\"\n",
    "    Recherche les passages les plus pertinents dans l'index FAISS.\n",
    "    \n",
    "    Args:\n",
    "    query (str): La requête de l'utilisateur.\n",
    "    index (faiss.IndexFlatL2): L'index FAISS.\n",
    "    model (SentenceTransformer): Modèle pour générer les embeddings.\n",
    "    chunks (list): Liste des morceaux de texte.\n",
    "    k (int): Nombre de passages à retourner.\n",
    "    \n",
    "    Returns:\n",
    "    list: Liste des passages pertinents.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# Tester avec une requête\n",
    "query = \"Qu'est ce qu'un RAG ?\"\n",
    "relevant_chunks = query_faiss_index(query, faiss_index, SentenceTransformer('all-MiniLM-L6-v2'), pdf_chunks)\n",
    "print(\"Passages pertinents :\")\n",
    "for chunk in relevant_chunks:\n",
    "    print(\"-\" * 80)\n",
    "    print(chunk)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50536e72bf4ba48a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Module 4 : Générer une réponse avec un LLM\n",
    "def generate_answer(query, relevant_chunks):\n",
    "    \"\"\"\n",
    "    Génère une réponse à partir de la question et des passages pertinents.\n",
    "    \n",
    "    Args:\n",
    "    query (str): La question posée.\n",
    "    relevant_chunks (list): Passages pertinents.\n",
    "    \n",
    "    Returns:\n",
    "    str: Réponse générée.\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join(relevant_chunks)\n",
    "    prompt = f\"Voici des informations utiles :\\n{context}\\n\\nQuestion : {query}\\nRéponse :\"\n",
    "    \n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Tu es un assistant RAG spécialisé dans la rédaction de réponse.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    return(completion.choices[0].message.content)\n",
    "\n",
    "# Tester la génération de réponse\n",
    "response = generate_answer(query, relevant_chunks)\n",
    "print(\"Réponse générée :\")\n",
    "print(response)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e702d0b4e58f115"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Langchain\n",
    "**Objectif** : faire une requête au modèle 'gpt4o-mini' via la librairie Langchain et récupérer la réponse "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b967dd1bc6dce1cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60ab86e73bae2fec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gpt_langchain(question):\n",
    "    \"\"\"\n",
    "    Prend une question et retourne la réponse générée via langchain\n",
    "    \n",
    "    Args: \n",
    "        question(str) : question à poser\n",
    "    \"\"\"\n",
    "        \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    prompt_template = PromptTemplate.from_template(question)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    \n",
    "    return(chain.invoke({}))\n",
    "\n",
    "anwser = ask_gpt_langchain(\"Qu'est ce qu'un transformer ?\")\n",
    "print(anwser)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lecture d'un PDF et utilisation dans RAG avec LangChain\n",
    "**Objectif** : Construire une architecture RAG avec langchain via 3 modules : \n",
    "1 - récupération du document via PyPDFLoader\n",
    "2 - génération du retriever après avoir split le document\n",
    "3 - Création de la chaîne de réponse à la question\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49bf6056"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_openai import OpenAIEmbeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4da9a3d3886988a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_docs(pdf_path):\n",
    "    \"\"\"\n",
    "    Charge et extrait le contenu textuel d'un fichier PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Chemin vers le fichier PDF à charger.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste contenant le contenu textuel de chaque page du PDF.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    return(docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fef1ac6bfe8ca792"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_retriever(docs):\n",
    "    \"\"\"\n",
    "    Crée un récupérateur (retriever) basé sur les embeddings pour une recherche de similarité.\n",
    "\n",
    "    Args:\n",
    "        docs (list): Liste des documents à traiter\n",
    "\n",
    "    Returns:\n",
    "        BaseRetriever\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, \n",
    "                                                   chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    vectorstore = InMemoryVectorStore.from_documents(documents=splits, \n",
    "                                                     embedding=OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cc4c3cf808a19f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_answer(question, retriever) :\n",
    "    \"\"\"\n",
    "    Génère une réponse à une question en utilisant une architecture RAG.\n",
    "\n",
    "    Args:\n",
    "        question (str): La question posée.\n",
    "        retriever (BaseRetriever): Un récupérateur permettant de chercher le contexte pertinent\n",
    "                                   pour répondre à la question.\n",
    "\n",
    "    Returns:\n",
    "        str: La réponse générée par le modèle, basée sur le contexte récupéré. \n",
    "    \"\"\"\n",
    "    system_prompt = (\"You are an assistant for question-answering tasks. \"\n",
    "                     \"Use the following pieces of retrieved context to answer \"\n",
    "                     \"the question. If you don't know the answer, say that you \"\n",
    "                     \"don't know. Use three sentences maximum and keep the \"\n",
    "                     \"answer concise.\"\n",
    "                     \"\\n\\n\"\n",
    "                     \"{context}\")\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt),\n",
    "                                               (\"human\", \"{input}\"),])\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "    \n",
    "    results = rag_chain.invoke({\"input\": f\"{question}\"})\n",
    "    \n",
    "    return(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7640393926e24d78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs = get_docs('../Data/Cours_LLM.pdf')\n",
    "retriever = get_retriever(docs)\n",
    "results = generate_answer(\"Qu'est ce qu'un RAG ?\", retriever)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3c509ef9aeb7a02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca343056a836af6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(results[\"context\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fee632557fc2dd98"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abcbaef07a026085"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='mistral', messages=[\n",
    "   {\n",
    "     'role': 'user',\n",
    "     'content': 'Talk about Data Science',\n",
    "   },\n",
    " ], stream=True)\n",
    "\n",
    "for chunk in response:    \n",
    "    print(chunk[\"message\"][\"content\"], end='', flush=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "829de9b16e1c52bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6321aa96df343814"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
