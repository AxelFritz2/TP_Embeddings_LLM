{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TP : Représentation et Visualisation de mots avec Word2Vec et t-SNE\n",
    "\n",
    "Objectifs \n",
    "1. Extraire et tokeniser le texte d'un document PDF.\n",
    "2. Représenter les mots de façon vectorielle (embeddings).\n",
    "3. Visualiser ces représentations en 2D avec **t-SNE**.\n",
    "4. Entraîner notre propre modèle Word2Vec et comparer avec un modèle pré-entraîné.\n",
    "5. Ajouter une classification non supervisée pour faire apparaître des classes de mots.\n",
    "\n",
    "Packages Nécessaires : \n",
    "```\n",
    "pip install gensim\n",
    "pip install matplotlib\n",
    "pip install scikit-learn \n",
    "pip install nltk \n",
    "pip install PyMuPDF\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cfd3e46667d4cf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import fitz\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim.downloader as api\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e43d38b373910b2d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partie 1 : Extraction et Visualisation avec un modèle pré-entraîné\n",
    "\n",
    "Dans cette première partie :\n",
    "- Charger un fichier PDF\n",
    "- Tokeniser le texte en mots\n",
    "- Utiliser le modèle **Word2Vec Google News** pré-entraîné\n",
    "- Réduire la dimension avec t-SNE\n",
    "- Visualiser les mots en 2D\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e4527cf3c2c3eca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21532ec822520c17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Extraction du texte PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extrait le texte brut d'un fichier PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Chemin vers le fichier PDF.\n",
    "\n",
    "    Returns:\n",
    "        str: Texte concaténé de toutes les pages du PDF.\n",
    "    \"\"\"\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b387b8e6e9b5d36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Tokenisation simple\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenise un texte en mots, en supprimant la ponctuation et les chiffres.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texte à tokeniser.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste des mots en minuscules, uniquement alphabétiques.\n",
    "    \"\"\"\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22470c592c5646ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Charger un modèle pré-entraîné (Google News)\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e83d49fcf951470c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Générér les Embeddings\n",
    "def generates_embeddings(tokens):\n",
    "    \"\"\"Génère les vecteurs d'embedding pour une liste de tokens en utilisant un modèle Word2Vec existant.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): Liste de mots (tokens) extraits d'un texte.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - embeddings (list): Liste des vecteurs d'embedding pour les mots présents dans le modèle.\n",
    "            - words_in_model (list): Liste des mots qui existent dans le vocabulaire du modèle.\n",
    "    \"\"\"\n",
    "    return embeddings, words_in_model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc7a3dc1c0363ec5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Pipeline\n",
    "pdf_path = \"./cours_LLM.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "tokens = tokenize(text)\n",
    "embeddings, words_in_model = generates_embeddings(tokens)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea874c326bebdeca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 5. Réduction de dimension : Entraîner un modèle type TSNE pour récupérer la représentation 2D des embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1280abe96934f587"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# 6. Visualisation : Représenter les 100 premiers mots des embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-16T18:27:08.751744Z",
     "start_time": "2025-09-16T18:27:08.744900Z"
    }
   },
   "id": "654ad71934e1c952"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partie 2 : Entraîner un modèle Word2Vec (CBOW) sur le PDF\n",
    "\n",
    "Dans cette partie :\n",
    "- Améliorer la tokenisation (enlever les stop words, ...) \n",
    "- Entraîner un modèle **Word2Vec CBOW** directement sur le contenu du PDF\n",
    "- Visualiser les résultats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21283c89c1a9a5a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdc1afe995f82ad0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Tokenisation + nettoyage\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenise et nettoie un texte en supprimant la ponctuation, les chiffres et les stopwords.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texte brut à tokeniser.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste des mots en minuscules, uniquement alphabétiques et sans stopwords.\n",
    "    \"\"\"\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "550b9da5c60a4d73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Entraînement Word2Vec (CBOW)\n",
    "def train_word2vec(tokens):\n",
    "    \"\"\"Entraîne un modèle Word2Vec en utilisant l'architecture CBOW sur une liste de tokens.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): Liste de mots (tokens) prétraités.\n",
    "\n",
    "    Returns:\n",
    "        gensim.models.Word2Vec: Modèle Word2Vec entraîné.\n",
    "    \"\"\"\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f830560861312c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3. Extraire embeddings\n",
    "def get_embeddings(model, top_n=100):\n",
    "    \"\"\"Extrait les embeddings des mots les plus fréquents du modèle Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        model (gensim.models.Word2Vec): Modèle Word2Vec entraîné.\n",
    "        top_n (int, optional): Nombre maximum de mots à extraire. \n",
    "            Par défaut 100.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - words (list): Liste des mots extraits.\n",
    "            - vectors (numpy.ndarray): Matrice des vecteurs correspondants (shape: top_n x dimension).\n",
    "    \"\"\"\n",
    "    return words, vectors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25ebd46465572f63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 4. Réduction et visualisation\n",
    "def reduce_with_tsne(vectors, words):\n",
    "   \"\"\"Réduit la dimension des embeddings de mots en 2D avec t-SNE.\n",
    "\n",
    "    Args:\n",
    "        vectors (numpy.ndarray): Matrice des vecteurs de mots.\n",
    "        words (list): Liste des mots correspondants aux vecteurs.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Coordonnées 2D des mots après réduction (shape: n_words x 2).\n",
    "    \"\"\"\n",
    "    return representation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3a48da39d2e99b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_embeddings(reduced, words, title=\"t-SNE (Word2Vec CBOW)\"):\n",
    "    \"\"\"Affiche une projection 2D des mots après réduction de dimension.\n",
    "\n",
    "    Args:\n",
    "        reduced (numpy.ndarray): Coordonnées 2D des mots (shape: n_words x 2).\n",
    "        words (list): Liste des mots correspondants aux coordonnées.\n",
    "        title (str, optional): Titre du graphique. \n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1c99da379a111f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pipeline Finale\n",
    "text = extract_text_from_pdf(pdf_path) \n",
    "tokens = tokenize_text(text)\n",
    "model = train_word2vec(tokens)\n",
    "words, vectors = get_embeddings(model, top_n=100)\n",
    "reduced = reduce_with_tsne(vectors, words)\n",
    "plot_embeddings(reduced, words, \"Projection t-SNE (Word2Vec CBOW, PDF)\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "465f075c7c44d589"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Partie 3 : Classification non supervisée des mots\n",
    "\n",
    "Pour mieux comprendre la structure des mots :\n",
    "- Entraînement d'un clustering **KMeans** sur les vecteurs Word2Vec\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "351241868105922"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Clustering\n",
    "def cluster_embeddings(vectors, n_clusters=5):\n",
    "    \"\"\"Applique un clustering KMeans sur les vecteurs de mots.\n",
    "\n",
    "    Args:\n",
    "        vectors (numpy.ndarray): Matrice des vecteurs de mots (shape: n_words x dimension).\n",
    "        n_clusters (int, optional): Nombre de clusters à générer. \n",
    "            Défaut = 5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Tableau des labels de clusters attribués à chaque mot (shape: n_words,).\n",
    "    \"\"\"\n",
    "    return kmeans.fit_predict(vectors)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ccff38d69c906e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Visualisation avec couleurs par cluster\n",
    "def plot_embeddings_clusters(reduced, words, labels, title=\"t-SNE avec clustering\"):\n",
    "    \"\"\"Affiche la projection 2D des mots avec une coloration par cluster.\n",
    "\n",
    "    Args:\n",
    "        reduced (numpy.ndarray): Coordonnées 2D des mots (shape: n_words x 2).\n",
    "        words (list): Liste des mots correspondants aux coordonnées.\n",
    "        labels (numpy.ndarray): Tableau des labels de clusters attribués à chaque mot (shape: n_words,).\n",
    "        title (str, optional): Titre du graphique. \n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25679ae1704f145c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pipeline Finale\n",
    "labels = cluster_embeddings(vectors, n_clusters=5)\n",
    "plot_embeddings_clusters(reduced, words, labels, \"Projection t-SNE (Word2Vec CBOW + KMeans)\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e2e1c98721f81cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
